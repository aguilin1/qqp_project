{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 4,
>>>>>>> 77dbdd674af7da3afce984eb3d206f590084662f
   "id": "978f956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Collecting pandas\n",
      "  Using cached pandas-1.4.3-cp39-cp39-macosx_11_0_arm64.whl (10.5 MB)\n",
      "Requirement already satisfied: numpy in /Users/annguilinger/miniforge3/lib/python3.9/site-packages (1.22.4)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gensim\n",
      "  Using cached gensim-4.2.0.tar.gz (23.2 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyemd\n",
      "  Using cached pyemd-0.5.1.tar.gz (91 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: keras in /Users/annguilinger/miniforge3/lib/python3.9/site-packages (2.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/annguilinger/miniforge3/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.2.1-py2.py3-none-any.whl (500 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.6/500.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2022.7.25-cp39-cp39-macosx_11_0_arm64.whl (282 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.6/282.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Requirement already satisfied: tqdm in /Users/annguilinger/miniforge3/lib/python3.9/site-packages (from nltk) (4.64.0)\n",
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.1.2-cp39-cp39-macosx_12_0_arm64.whl (7.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=0.18.1\n",
      "  Downloading scipy-1.9.0-cp39-cp39-macosx_12_0_arm64.whl (29.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.9/29.9 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting smart_open>=1.8.1\n",
      "  Using cached smart_open-6.0.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/annguilinger/miniforge3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: sklearn, gensim, pyemd\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1304 sha256=bd1184be04f523d015e70832d88f770517d316d1f7bc766632e58017d67414e3\n",
      "  Stored in directory: /Users/annguilinger/Library/Caches/pip/wheels/e4/7b/98/b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "  Building wheel for gensim (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gensim: filename=gensim-4.2.0-cp39-cp39-macosx_11_0_arm64.whl size=23932954 sha256=e6aa9a3af83018e63393065b5a9f805409107872f01ca38cc58cc90166908087\n",
      "  Stored in directory: /Users/annguilinger/Library/Caches/pip/wheels/ed/5e/79/d2997e72ba8900a820dd5870a3566779e52ee8279f71b4c799\n",
      "  Building wheel for pyemd (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyemd: filename=pyemd-0.5.1-cp39-cp39-macosx_11_0_arm64.whl size=68939 sha256=08fcb0b0e01238d83387957b93b30afc90778627b1ce9d68a87628dbe8f24969\n",
      "  Stored in directory: /Users/annguilinger/Library/Caches/pip/wheels/64/bf/3e/0859be9a0108fc932a29b943792dcafb3b979555cf1bb5add6\n",
      "Successfully built sklearn gensim pyemd\n",
      "Installing collected packages: pytz, threadpoolctl, smart_open, scipy, regex, pyemd, joblib, click, scikit-learn, pandas, nltk, gensim, sklearn\n",
      "Successfully installed click-8.1.3 gensim-4.2.0 joblib-1.1.0 nltk-3.7 pandas-1.4.3 pyemd-0.5.1 pytz-2022.2.1 regex-2022.7.25 scikit-learn-1.1.2 scipy-1.9.0 sklearn-0.0 smart_open-6.0.0 threadpoolctl-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
=======
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/annguilinger/Library/Python/3.8/lib/python/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy in /Users/annguilinger/Library/Python/3.8/lib/python/site-packages (1.22.2)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /Users/annguilinger/Library/Python/3.8/lib/python/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/annguilinger/Library/Python/3.8/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.7.25-cp38-cp38-macosx_11_0_arm64.whl (282 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.6/282.6 KB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /Users/annguilinger/Library/Python/3.8/lib/python/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in /Users/annguilinger/Library/Python/3.8/lib/python/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: tqdm in /Users/annguilinger/Library/Python/3.8/lib/python/site-packages (from nltk) (4.63.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
      "Installing collected packages: regex, nltk\n",
      "\u001b[33m  WARNING: The script nltk is installed in '/Users/annguilinger/Library/Python/3.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed nltk-3.7 regex-2022.7.25\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
>>>>>>> 77dbdd674af7da3afce984eb3d206f590084662f
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "pip install pandas numpy nltk sklearn gensim pyemd keras"
=======
    "pip install pandas numpy nltk"
>>>>>>> 77dbdd674af7da3afce984eb3d206f590084662f
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285af388",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "We do some basic data cleaning including stemming (i.e. removing suffixes) and removing common words, tagging parts of speech, and finding the duplicate words between given question pairs"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 53,
>>>>>>> 77dbdd674af7da3afce984eb3d206f590084662f
   "id": "f53bc7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/annguilinger/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/annguilinger/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/annguilinger/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "expected string or bytes-like object\n",
=======
>>>>>>> 77dbdd674af7da3afce984eb3d206f590084662f
      "Ran into problem with data, removing question:\n",
      "id                                      105780\n",
      "qid1                                    174363\n",
      "qid2                                    174364\n",
      "question1       How can I develop android app?\n",
      "question2                                  NaN\n",
      "is_duplicate                                 0\n",
      "Name: 105780, dtype: object\n",
<<<<<<< HEAD
      "At least one of the passed list is empty.\n",
      "Ran into problem with data, removing question:\n",
      "id                                           108978\n",
      "qid1                                         178936\n",
      "qid2                                         178937\n",
      "question1                                         i\n",
      "question2       What questions to ask any drdummer?\n",
      "is_duplicate                                      0\n",
      "Name: 108978, dtype: object\n",
      "At least one of the passed list is empty.\n",
      "Ran into problem with data, removing question:\n",
      "id                                                       115347\n",
      "qid1                                                     188110\n",
      "qid2                                                      52215\n",
      "question1                                                     o\n",
      "question2       Where can I watch free streaming movies online?\n",
      "is_duplicate                                                  0\n",
      "Name: 115347, dtype: object\n",
      "At least one of the passed list is empty.\n",
      "Ran into problem with data, removing question:\n",
      "id                                                         151922\n",
      "qid1                                                       188110\n",
      "qid2                                                       238787\n",
      "question1                                                       o\n",
      "question2       What is this - “This website/URL has been bloc...\n",
      "is_duplicate                                                    0\n",
      "Name: 151922, dtype: object\n",
      "At least one of the passed list is empty.\n",
      "Ran into problem with data, removing question:\n",
      "id                     198913\n",
      "qid1                   300250\n",
      "qid2                   188110\n",
      "question1       What is this?\n",
      "question2                   o\n",
      "is_duplicate                0\n",
      "Name: 198913, dtype: object\n",
      "expected string or bytes-like object\n",
=======
>>>>>>> 77dbdd674af7da3afce984eb3d206f590084662f
      "Ran into problem with data, removing question:\n",
      "id                                        201841\n",
      "qid1                                      303951\n",
      "qid2                                      174364\n",
      "question1       How can I create an Android app?\n",
      "question2                                    NaN\n",
      "is_duplicate                                   0\n",
      "Name: 201841, dtype: object\n",
<<<<<<< HEAD
      "expected string or bytes-like object\n",
=======
>>>>>>> 77dbdd674af7da3afce984eb3d206f590084662f
      "Ran into problem with data, removing question:\n",
      "id                                                         363362\n",
      "qid1                                                       493340\n",
      "qid2                                                       493341\n",
      "question1                                                     NaN\n",
      "question2       My Chinese name is Haichao Yu. What English na...\n",
      "is_duplicate                                                    0\n",
<<<<<<< HEAD
      "Name: 363362, dtype: object\n",
      "At least one of the passed list is empty.\n",
      "Ran into problem with data, removing question:\n",
      "id                                                         381124\n",
      "qid1                                                       512812\n",
      "qid2                                                       512813\n",
      "question1                                                      no\n",
      "question2       I have a BS and MPH and hate my job. I found t...\n",
      "is_duplicate                                                    0\n",
      "Name: 381124, dtype: object\n"
=======
      "Name: 363362, dtype: object\n"
>>>>>>> 77dbdd674af7da3afce984eb3d206f590084662f
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import *\n",
    "from nltk.corpus import stopwords\n",
    "\n",
<<<<<<< HEAD
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
=======
>>>>>>> 77dbdd674af7da3afce984eb3d206f590084662f
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
<<<<<<< HEAD
    "import gensim.downloader as api\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\") \n",
    "\n",
=======
>>>>>>> 77dbdd674af7da3afce984eb3d206f590084662f
    "data_file_location = \"./quora_duplicate_questions.tsv\"\n",
    "data = pd.read_csv(\n",
    "    data_file_location,\n",
    "    sep='\\t',\n",
    ")\n",
    "\n",
    "stemmed_q1s = []\n",
    "tagged_q1s = []\n",
    "stemmed_q2s = []\n",
    "tagged_q2s = []\n",
    "dups_all = []\n",
<<<<<<< HEAD
    "cs_all = []\n",
=======
>>>>>>> 77dbdd674af7da3afce984eb3d206f590084662f
    "\n",
    "\n",
    "common_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
<<<<<<< HEAD
    "vectorizer = CountVectorizer()\n",
=======
>>>>>>> 77dbdd674af7da3afce984eb3d206f590084662f
    "\n",
    "for index, question in data.iterrows():\n",
    "    question1 = question.question1\n",
    "    question2 = question.question2\n",
    "\n",
    "    try:\n",
    "        tokens1 = [token for token in wordpunct_tokenize(question1) if token not in common_words]\n",
    "        stemmed1 = [stemmer.stem(word) for word in tokens1]\n",
    "        tagged1 = nltk.pos_tag(stemmed1)\n",
    "\n",
    "        tokens2 = [token for token in wordpunct_tokenize(question2) if token not in common_words]\n",
    "        stemmed2 = [stemmer.stem(word) for word in tokens2]\n",
    "        tagged2 = nltk.pos_tag(stemmed2)\n",
    "        \n",
    "        dups = [word for word in stemmed1 if word in stemmed2]\n",
<<<<<<< HEAD
    "        try:\n",
    "            cs = word_vectors.n_similarity(stemmed1,stemmed2)\n",
    "        except ValueError as er:\n",
    "            print(dups)\n",
    "            print(er)\n",
    "            cs = 0\n",
    "        \n",
    "        stemmed_q1s.append(stemmed1)\n",
    "        tagged_q1s.append(tagged1)\n",
    "        stemmed_q2s.append(stemmed2)\n",
    "        tagged_q2s.append(tagged2)\n",
    "        dups_all.append(dups)\n",
    "        cs_all.append(cs)\n",
    "\n",
    "    except Exception as e:\n",
    "        data.drop([index],inplace=True)\n",
    "        print(e)\n",
    "\n",
    "        print(\"Ran into problem with data, removing question:\")\n",
    "        print(question)\n",
    "        continue\n",
=======
    "    except:\n",
    "        data.drop([index],inplace=True)\n",
    "        print(\"Ran into problem with data, removing question:\")\n",
    "        print(question)\n",
    "        continue\n",
    "    stemmed_q1s.append(stemmed1)\n",
    "    tagged_q1s.append(tagged1)\n",
    "    stemmed_q2s.append(stemmed2)\n",
    "    tagged_q2s.append(tagged2)\n",
    "    dups_all.append(dups)\n",
    "\n",
>>>>>>> 77dbdd674af7da3afce984eb3d206f590084662f
    "data.insert(4,'q1_stems',stemmed_q1s)\n",
    "data.insert(5,'q1_tags',tagged_q1s)\n",
    "data.insert(7,'q2_stems',stemmed_q2s)\n",
    "data.insert(8,'q2_tags',tagged_q2s)\n",
<<<<<<< HEAD
    "data.insert(9,'duplicates',dups_all)\n",
    "data.insert(10,'cosine_similarity',cs_all)"
=======
    "data.insert(9,'duplicates',dups_all)"
>>>>>>> 77dbdd674af7da3afce984eb3d206f590084662f
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 56,
>>>>>>> 77dbdd674af7da3afce984eb3d206f590084662f
   "id": "fae80d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  qid1  qid2                                          question1  \\\n",
      "0   0     1     2  What is the step by step guide to invest in sh...   \n",
      "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
      "2   2     5     6  How can I increase the speed of my internet co...   \n",
      "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
      "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
      "\n",
      "                                            q1_stems  \\\n",
      "0  [what, step, step, guid, invest, share, market...   \n",
      "1  [what, stori, kohinoor, (, koh, -, -, noor, ),...   \n",
      "2  [how, i, increas, speed, internet, connect, us...   \n",
      "3         [whi, i, mental, lone, ?, how, i, solv, ?]   \n",
      "4  [which, one, dissolv, water, quikli, sugar, ,,...   \n",
      "\n",
      "                                             q1_tags  \\\n",
      "0  [(what, WP), (step, VB), (step, NN), (guid, NN...   \n",
      "1  [(what, WP), (stori, VBD), (kohinoor, NN), ((,...   \n",
      "2  [(how, WRB), (i, JJ), (increas, VBP), (speed, ...   \n",
      "3  [(whi, NN), (i, NN), (mental, VBP), (lone, NN)...   \n",
      "4  [(which, WDT), (one, CD), (dissolv, NN), (wate...   \n",
      "\n",
      "                                           question2  \\\n",
      "0  What is the step by step guide to invest in sh...   \n",
      "1  What would happen if the Indian government sto...   \n",
      "2  How can Internet speed be increased by hacking...   \n",
      "3  Find the remainder when [math]23^{24}[/math] i...   \n",
      "4            Which fish would survive in salt water?   \n",
      "\n",
      "                                            q2_stems  \\\n",
      "0  [what, step, step, guid, invest, share, market...   \n",
      "1  [what, would, happen, indian, govern, stole, k...   \n",
      "2       [how, internet, speed, increas, hack, dn, ?]   \n",
      "3  [find, remaind, [, math, ], 23, ^{, 24, }[/, m...   \n",
      "4       [which, fish, would, surviv, salt, water, ?]   \n",
      "\n",
      "                                             q2_tags  \\\n",
      "0  [(what, WP), (step, VB), (step, NN), (guid, NN...   \n",
      "1  [(what, WP), (would, MD), (happen, VB), (india...   \n",
      "2  [(how, WRB), (internet, JJ), (speed, NN), (inc...   \n",
      "3  [(find, VB), (remaind, NN), ([, JJ), (math, NN...   \n",
      "4  [(which, WDT), (fish, NN), (would, MD), (survi...   \n",
      "\n",
<<<<<<< HEAD
      "                                          duplicates  cosine_similarity  \\\n",
      "0  [what, step, step, guid, invest, share, market...           0.985732   \n",
      "1  [what, kohinoor, (, koh, -, -, noor, ), diamon...           0.949585   \n",
      "2                 [how, increas, speed, internet, ?]           0.929641   \n",
      "3                                             [?, ?]           0.757050   \n",
      "4                            [which, water, salt, ?]           0.942189   \n",
      "\n",
      "   is_duplicate  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n"
=======
      "                                          duplicates  is_duplicate  \n",
      "0  [what, step, step, guid, invest, share, market...             0  \n",
      "1  [what, kohinoor, (, koh, -, -, noor, ), diamon...             0  \n",
      "2                 [how, increas, speed, internet, ?]             0  \n",
      "3                                             [?, ?]             0  \n",
      "4                            [which, water, salt, ?]             0  \n"
>>>>>>> 77dbdd674af7da3afce984eb3d206f590084662f
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "id": "1e7b7d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:,:-1],data.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b926c1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 12\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m           \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m           \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(500, 128, input_length=100))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "res = model.fit(X_train, y_train,\n",
    "           batch_size=50,\n",
    "           epochs=12,\n",
    "           validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165e47b",
=======
   "execution_count": null,
   "id": "1b926c1f",
>>>>>>> 77dbdd674af7da3afce984eb3d206f590084662f
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.9.13"
=======
   "version": "3.8.9"
>>>>>>> 77dbdd674af7da3afce984eb3d206f590084662f
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
